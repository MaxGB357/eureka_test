# Voice Agent Configuration Guide

This guide explains how to configure your OpenAI Realtime Voice Agent, including session settings, voice options, and advanced parameters.

---

## Table of Contents

1. [Voice Configuration](#voice-configuration)
2. [Session Configuration](#session-configuration)
3. [User Transcription](#user-transcription)
4. [Agent Configuration](#agent-configuration)
5. [Model Options](#model-options)
6. [Advanced Settings](#advanced-settings)

---

## Voice Configuration

### Available Voices

The OpenAI Realtime API supports multiple voice options. Configure the voice in `public/agent.js`:

```javascript
const agent = new RealtimeAgent({
  name: 'Helpful Assistant',
  instructions: 'You are a helpful voice assistant...',
  tools: [getWeatherTool, getCurrentTimeTool, calculateTool],
  voice: 'verse',  // Change this to your preferred voice
  temperature: 0.8,
});
```

### Voice Options:

| Voice | Description |
|-------|-------------|
| `alloy` | Neutral, balanced voice |
| `echo` | Clear, well-rounded voice |
| `shimmer` | Warm, engaging voice |
| `verse` | Articulate, expressive voice |
| `ash` | Clear, conversational voice |

### How to Change Voice:

1. Open `public/agent.js`
2. Find line 72: `voice: 'verse',`
3. Replace `'verse'` with your preferred voice from the list above
4. Save the file
5. Refresh your browser (hard refresh: Ctrl+Shift+R)

---

## Session Configuration

### Backend Session Settings

The backend generates ephemeral keys with specific session parameters. Configure these in `src/server.ts` at line 36-41:

```typescript
const sessionConfig = {
  session: {
    type: 'realtime',
    model: 'gpt-4o-realtime-preview'
  }
};
```

### Current Session Structure:

- **type**: Must be `'realtime'` for Realtime API
- **model**: The AI model to use (see [Model Options](#model-options))

**Note:** User audio transcription is automatically enabled by OpenAI and cannot be configured via the ephemeral key endpoint.

### Optional Session Parameters:

**Note:** The `/v1/realtime/client_secrets` endpoint has limited configuration options. Most session parameters (like turn detection, audio formats, etc.) cannot be set here. They are either:
- Configured automatically by OpenAI
- Set via the WebSocket connection after the session starts
- Configured client-side in the RealtimeAgent

The ephemeral key endpoint primarily accepts:
- `type`: Must be `'realtime'`
- `model`: The AI model to use

For advanced configuration, you may need to use the WebSocket connection directly after establishing the session.

**Note**: After modifying `server.ts`, you must:
1. Stop the server (Ctrl+C)
2. Rebuild: `npm run build`
3. Restart: `npm start`

---

## User Transcription

### What is User Transcription?

User transcription converts your spoken words into text that appears in the **Conversation Transcript** panel. By default, the agent can hear and respond to you, but your words won't appear as text unless transcription is enabled.

### Enabling User Transcription

User transcription is **ENABLED BY DEFAULT** in the OpenAI Realtime API. You don't need to configure it manually - OpenAI automatically transcribes your audio input using Whisper.

**Note:** The transcription cannot be configured when creating the ephemeral key. OpenAI handles this automatically on their end.

### How It Works

1. **You speak** → Microphone captures audio
2. **Audio sent to OpenAI** → Realtime API receives audio stream
3. **Whisper transcribes** → Your speech is converted to text
4. **Text appears in UI** → Shows as "User: [your words]" in the transcript panel

### Viewing Transcripts

Once enabled, you'll see your messages in two places:

**Conversation Transcript Panel:**
```
User: What's the weather like today?
Agent: I'd be happy to help you check the weather. What city are you interested in?
```

**Event Log:**
```
[12:34:56] You: What's the weather like today?
[12:34:58] Agent is responding...
[12:34:59] Agent: I'd be happy to help you check...
```

### Disabling User Transcription

**Note:** Currently, transcription cannot be disabled via the ephemeral key endpoint. OpenAI enables it by default. If you want to hide transcripts in the UI (but they will still be generated by OpenAI), you can remove the transcript event listeners from `agent.js`.

The agent will always hear and respond to you - the transcription just makes your words visible as text in the UI.

### Transcription Models

Currently available models:

| Model | Description | Cost | Speed |
|-------|-------------|------|-------|
| `whisper-1` | Standard Whisper model | Standard pricing | Fast |

OpenAI may add more transcription models in the future. Check the [OpenAI Pricing page](https://openai.com/pricing) for current costs.

### Troubleshooting Transcripts

**Issue: I don't see my transcripts**

✅ **Solutions:**
1. Hard refresh browser (Ctrl+Shift+R)
2. Check browser console for:
   - `[Frontend] Transcription event:` logs
   - `[Frontend] Transcription completed:` logs
   - `[Frontend] Adding user transcript:` logs
3. Verify microphone is working and audio is being captured
4. Check for duplicate transcripts being skipped: `[Frontend] Skipping duplicate transcript:`
5. Make sure you're speaking clearly and waiting for the transcription to complete

**Issue: Transcripts are delayed**

This is normal - transcription happens after you finish speaking. The agent responds to audio in real-time, but the transcript appears slightly after.

**Issue: Transcripts are inaccurate**

Whisper works best with:
- Clear audio (reduce background noise)
- Normal speaking pace (not too fast or slow)
- Standard accents and dialects
- Good microphone quality

---

## Agent Configuration

### Basic Agent Setup

Configure the agent behavior in `public/agent.js`:

```javascript
const agent = new RealtimeAgent({
  name: 'Helpful Assistant',           // Agent's name
  instructions: 'System instructions',  // How the agent should behave
  tools: [tool1, tool2],               // Available tools/functions
  voice: 'verse',                       // Voice selection
  temperature: 0.8,                     // Response randomness (0.0-1.0)
});
```

### Configuration Options:

| Parameter | Type | Description | Default |
|-----------|------|-------------|---------|
| `name` | string | Agent's display name | Required |
| `instructions` | string | System prompt defining agent behavior | Required |
| `tools` | array | Array of tool definitions | `[]` |
| `voice` | string | Voice selection (see above) | `'alloy'` |
| `temperature` | number | Response creativity (0.0 = focused, 1.0 = creative) | `0.8` |
| `max_tokens` | number | Maximum response length | undefined |

### Example: Changing Temperature

```javascript
const agent = new RealtimeAgent({
  name: 'Precise Assistant',
  instructions: 'You provide accurate, factual responses.',
  tools: [],
  voice: 'ash',
  temperature: 0.2,  // Lower = more deterministic
});
```

---

## Model Options

### Available Models

Currently supported Realtime API models:

| Model ID | Description | Use Case |
|----------|-------------|----------|
| `gpt-4o-realtime-preview` | OpenAI Realtime model | Production use, automatically updated to latest version |

### How to Change Model:

**Option 1: Backend Configuration (Recommended)**

Edit `src/server.ts` line 39:

```typescript
model: 'gpt-4o-realtime-preview',
```

Then rebuild and restart the server.

**Option 2: Frontend Configuration**

You can also specify the model when creating the session in `agent.js`:

```javascript
const session = new RealtimeSession(agent, {
  model: 'gpt-4o-realtime-preview',
});
```

**Note**: Backend configuration is preferred as it ensures consistency and security.

---

## Advanced Settings

### Turn Detection Configuration

Turn detection controls when the AI responds to speech. Add to backend session config:

```typescript
turn_detection: {
  type: 'server_vad',         // Voice Activity Detection on server
  threshold: 0.5,              // Detection sensitivity (0.0-1.0)
  prefix_padding_ms: 300,      // Include audio before speech starts
  silence_duration_ms: 200     // Silence needed to end turn
}
```

**Parameters:**
- **threshold**: Lower = more sensitive (may interrupt more)
- **prefix_padding_ms**: Captures audio before detected speech
- **silence_duration_ms**: How long to wait before considering speech ended

### Audio Format Options

Configure audio input/output formats:

```typescript
input_audio_format: 'pcm16',   // Raw PCM 16-bit
output_audio_format: 'pcm16',

// Alternative formats:
// 'g711_ulaw' - μ-law encoding (smaller bandwidth)
// 'g711_alaw' - A-law encoding (smaller bandwidth)
```

### Session Events

The frontend can listen to various session events in `agent.js`:

```javascript
session.on('connected', () => {
  console.log('Session connected');
});

session.on('disconnected', () => {
  console.log('Session disconnected');
});

session.on('error', (error) => {
  console.error('Session error:', error);
});

session.on('user_transcript', (text) => {
  console.log('User said:', text);
});

session.on('agent_transcript', (text) => {
  console.log('Agent said:', text);
});

session.on('tool_call', (toolCall) => {
  console.log('Tool called:', toolCall.name);
});

// Additional events:
session.on('audio_buffer.speech_started', () => {
  console.log('User started speaking');
});

session.on('audio_buffer.speech_stopped', () => {
  console.log('User stopped speaking');
});

session.on('response.audio.delta', (audio) => {
  // Audio chunk received
});
```

### Custom Tool Configuration

Add custom tools to your agent:

```javascript
import { tool } from 'https://cdn.jsdelivr.net/npm/@openai/agents-realtime@0.1.0/+esm';
import { z } from 'https://cdn.jsdelivr.net/npm/zod@3.23.8/+esm';

const myCustomTool = tool({
  name: 'my_tool',
  description: 'Description of what this tool does',
  parameters: z.object({
    param1: z.string().describe('First parameter'),
    param2: z.number().optional().describe('Optional number'),
  }),
  execute: async ({ param1, param2 }) => {
    // Tool implementation
    return {
      result: 'success',
      data: 'some data'
    };
  },
});

// Add to agent:
const agent = new RealtimeAgent({
  name: 'Assistant',
  instructions: 'You can use my_tool to...',
  tools: [myCustomTool],
  voice: 'verse',
});
```

---

## Testing Your Configuration

After making changes:

1. **Backend changes** (`src/server.ts`):
   ```bash
   # Stop the server (Ctrl+C)
   npm run build
   npm start
   ```

2. **Frontend changes** (`public/agent.js`):
   ```bash
   # Just refresh your browser
   # Hard refresh: Ctrl+Shift+R (Windows/Linux) or Cmd+Shift+R (Mac)
   ```

3. **Verify in browser console**:
   - Look for `[Backend]` and `[Frontend]` debug logs
   - Check that ephemeral key starts with `ek_`
   - Verify connection succeeds

4. **Test voice interaction**:
   - Click "Connect & Start Talking"
   - Grant microphone access
   - Start speaking to test the agent

---

## Troubleshooting

### Issue: "Using the WebRTC connection requires an ephemeral client key"

**Solution**: Ensure you're using the correct endpoint and response format:
- Backend must call `/v1/realtime/client_secrets`
- Frontend must use `apiKey: sessionData.value`

### Issue: Voice sounds different than expected

**Solution**:
- Check `agent.js` line 72 for current voice setting
- Ensure voice name is spelled correctly
- Try a different voice from the list

### Issue: Agent doesn't respond

**Solution**:
- Check turn detection settings (might be too high threshold)
- Verify microphone permissions granted
- Check browser console for errors
- Ensure OpenAI API key has Realtime API access

### Issue: Connection fails

**Solution**:
- Verify `.env` has valid `OPENAI_API_KEY`
- Check backend console for `[Backend]` error logs
- Ensure server is running on port 3000
- Verify ephemeral key is being generated (check logs)

---

## Environment Variables

Required in `.env` file:

```bash
# OpenAI API Configuration
OPENAI_API_KEY=sk-proj-...your-key-here...

# Server Configuration
PORT=3000

# n8n Webhook Integration (for project submission)
N8N_WEBHOOK_URL=https://your-n8n-instance.app/webhook/submit-project
N8N_WEBHOOK_SECRET=your-webhook-secret-here
```

### Environment Variable Descriptions

| Variable | Required | Description | Example |
|----------|----------|-------------|---------|
| `OPENAI_API_KEY` | **Yes** | OpenAI API key for Realtime API access | `sk-proj-abc123...` |
| `PORT` | No | Local development server port | `3000` (default) |
| `N8N_WEBHOOK_URL` | **Yes*** | n8n webhook endpoint URL for project submission | `https://n8n.app/webhook/submit-project` |
| `N8N_WEBHOOK_SECRET` | **Yes*** | Secret token for webhook authentication | `my-secure-secret-123` |

\* Required only if using the project submission feature (submitProjectTool)

### Vercel Environment Variables

When deploying to Vercel, configure these variables in the Vercel Dashboard:

1. Go to **Project Settings** → **Environment Variables**
2. Add each variable:
   - `OPENAI_API_KEY` - Production, Preview, Development
   - `N8N_WEBHOOK_URL` - Production, Preview, Development
   - `N8N_WEBHOOK_SECRET` - Production, Preview, Development
3. Click **Save**
4. **Redeploy** the project (required for env vars to take effect)

### n8n Webhook Configuration

The `submitProjectTool` uses these environment variables to:

1. **Send project data to n8n**: POST request to `N8N_WEBHOOK_URL`
2. **Authenticate the webhook**: Sends `N8N_WEBHOOK_SECRET` in the `X-Webhook-Secret` header
3. **Save to Google Sheets**: n8n workflow saves the project data
4. **Send confirmation email**: n8n workflow sends email to the user

**Development vs Production**:

- **Development** (localhost): Uses `http://localhost:8080/webhook/submit-project` or env var if set
- **Production** (Vercel): Uses `process.env.N8N_WEBHOOK_URL`

**Security Note**: Never commit your `.env` file or expose your API keys in frontend code. The ephemeral key system ensures the main API key stays secure on the backend. The webhook secret should be a strong, random string.

---

## Additional Resources

- [OpenAI Realtime API Documentation](https://platform.openai.com/docs/guides/realtime)
- [OpenAI Agents SDK Documentation](https://openai.github.io/openai-agents-js/)
- [Voice Agents Quickstart](https://openai.github.io/openai-agents-js/guides/voice-agents/quickstart/)
- [Zod Schema Validation](https://zod.dev/) (for tool parameters)

---

## Quick Reference

### Change Voice
Location: `public/agent.js:72`
```javascript
voice: 'ash',  // alloy, echo, shimmer, verse, ash
```

### Change Model
Location: `src/server.ts:39`
```typescript
model: 'gpt-4o-realtime-preview',
```

### Change Temperature
Location: `public/agent.js:73`
```javascript
temperature: 0.5,  // 0.0 (focused) to 1.0 (creative)
```

### Change Agent Instructions
Location: `public/agent.js:64-70`
```javascript
instructions: 'Your custom system prompt here',
```
